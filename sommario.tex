\chapter*{Sommario} % senza numerazione
\label{sommario}

\addcontentsline{toc}{chapter}{Sommario} % da aggiungere comunque all'indice

% CONTESTO E MOTIVAZIONI

Negli ultimi anni, il Machine Learning è diventato una delle tecnologie più utilizzate in molti campi, dalla computer vision per il settore automobilistico al riconoscimento del linguaggio naturale per gli assistenti vocali. L'implementazione di modelli di Machine Learning su dispositivi con risorse limitate, come microcontrollori e sensori a bassa potenza, è stata tuttavia a lungo considerata una sfida tecnologica a causa delle limitazioni di memoria e potenza di elaborazione.

In questo contesto, è emersa la tecnologia del Tiny Machine Learning (TinyML) come soluzione per l'elaborazione di dati di sensori su dispositivi a capacità limitate. Il TinyML è l'apprendimento automatico implementato su dispositivi embedded a basso consumo energetico in grado di processare dati in tempo reale senza la necessità di trasferirli a un server remoto.

% CHECK Il Tiny Machine Learning offre numerosi vantaggi nell'elaborazione dei dati di sensori su dispositivi integrati a bassa potenza. Grazie alla capacità di elaborare i dati in tempo reale direttamente sul dispositivo, il TinyML elimina la necessità di trasferire i dati a terze parti per l'elaborazione, evitando ritardi di trasmissione e riducendo notevolmente la latenza. 

Il Tiny Machine Learning offre numerosi vantaggi nell'elaborazione dei dati di sensori su dispositivi integrati a bassa potenza; dato che non è neceassaria alcun tipo di connettività per l'inferenza, si possono elaborare i dati in tempo reale direttamnte sul dispositivo e si può evitare di trasferire dati grezzi a terze parti per l'elaborazione, ciò evita l'attesa di inutili ritardi di trasmissione delle informazioni e riduce notevolmetne la latenza. Per comprendere l'impatto della bassa latenza di risposta, basti pensare al monitoraggio continuo delle macchine in ambito industriale per prevederne i problemi ed i possibili guasti. Questo tipo di applicazione può dare una risposta tempestiva ai danni, riducendo i costi di manutenzione, i rischi di guasto e i tempi di inattività, oltre a migliorare le prestazioni e l'efficienza. 

I dispositivi integrati che supportano gli algoritmi di TinyML hanno bisogno di una quantità molto ridotta di potenza (nell'ordine dei mW), che consente loro di operare per lunghi periodi senza bisogno di essere caricati se provvisti di batteria, o con un consumo esiguo di energia se alimentati. Pensando invece a sistemi di video sorveglianza nelle smart cities, la possibilità di eseguire l'inferenza sul dispositivo senza la necessità di trasferire i dati per l'elaborazione a server esterni, oltre a diminuire i consumi di potenza dei dispositivi, aumenta il livello di privacy dei dati.

\section{Classificazione di immagini}

L'elaborazione e la classificazione di immagini sono diventate aree cruciali nel campo del machine learning. La capacità di comprendere e interpretare le informazioni visive è di fondamentale importanza in molti settori, quali la medicina, l'automazione industriale, la sorveglianza e molto altro.
La classificazione di immagini con tecniche di machine learning consente di addestrare algoritmi complessi per riconoscere e assegnare etichette a diverse categorie di immagini in modo automatico. L'obiettivo è quello di creare modelli che siano in grado di apprendere i pattern e le caratteristiche distintive presenti nelle immagini per effettuare previsioni accurate sulla classe di appartenenza di nuove immagini mai viste prima.

Le Convolutional Neural Networks sono tipicamente composte da strati di convoluzione, pooling e strati completamente connessi e sono progettate appositamente per estrarre le caratteristiche salienti dalle immagini. Attraverso la convoluzione e l'applicazione di filtri, le CNN catturano dettagli significativi, riducendo la complessità dei dati, tramite gli strati di pooling, si riducono le dimensioni delle feature map per concentrarsi sulle informazioni essenziali, infine gli strati completamente connessi combinano le caratteristiche estratte per effettuare la classificazione. Grazie al processo di apprendimento e all'ottimizzazione dei pesi, le CNN sono in grado di affinare le loro prestazioni nel riconoscimento e nella classificazione di immagini.
%Le Convolutional Neural Networks (CNN) sono progettate specificamente per l'elaborazione di dati visivi e sono in grado di estrarre automaticamente le caratteristiche rilevanti dalle immagini attraverso l'apprendimento dai dati.
%Le Convolutional Neural Networks (CNN) sono progettate specificamente per l'elaborazione di dati visivi e sono in grado di estrarre automaticamente le caratteristiche rilevanti dalle immagini attraverso diverse operazioni di convoluzione successive.
L'intero processo richiede un'adeguata preparazione dei dati di addestramento (dataset), che devono essere etichettati correttamente; successivamente le immagini vengono suddivise in un set di addestramento, un set di validazione e un set di test. Durante la fase di addestramento, le CNN sono sottoposte a un processo iterativo in cui i pesi dei vari strati vengono regolati per ridurre l'errore tra le etichette reali e le previsioni della rete; ciò avviene mediante l'ottimizzazione di una funzione di perdita, come ad esempio la \textit{cross-entropy loss}.
%
Una volta che la rete è stata addestrata, viene valutata utilizzando il set di validazione per misurare la sua capacità di generalizzazione; infine il modello viene testato sul set di test per valutare le sue prestazioni finali, ovvero l'accuratezza nella classificazione di nuove immagini.
%
La classificazione di immagini con tecniche di machine learning offre numerosi vantaggi, come l'automazione dei compiti di classificazione che richiederebbero molto tempo se eseguiti manualmente; inoltre permette di ottenere previsioni precise e affidabili, contribuendo a migliorare l'efficienza e la qualità delle attività che coinvolgono l'elaborazione di grandi quantità di immagini.

Tuttavia, è importante sottolineare che i modelli di classificazione di immagini più performanti spesso richiedono una grande quantità di risorse computazionali, tra cui potenza di calcolo e memoria. Questo aspetto rappresenta una sfida significativa nell'implementazione pratica di tali modelli, specialmente su dispositivi con risorse limitate come i microcontrollori.
Qui sorge l'importanza del Tiny Machine Learning, che si propone di implementare modelli di machine learning anche su dispositivi a capacità computazionali limitate.

%\section{Tecniche di implementazione di TinyML}
\section{problemi/soluzioni tinyML}

Una delle sfide principali che i ricercatori e gli ingegneri affrontano nell'implementazione del Tiny Machine Learning è l'eterogeneità delle piattaforme. A causa della grande varietà di microcontrollori, sensori e dispositivi integrati disponibili sul mercato, c'è una notevole diversità di architetture hardware e software che può rendere difficile lo sviluppo e la distribuzione di soluzioni TinyML su una vasta gamma di dispositivi. 

L'eterogeneità delle piattaforme può causare problemi di compatibilità tra hardware e software e può rendere difficile la creazione di modelli che funzionano in modo affidabile su più dispositivi, ciò può comportare l'implementazione di soluzioni specifiche per singoli dispositivi, riducendo l'efficienza del processo di sviluppo ed aumentandone i costi.

Inoltre, i microcontrollori spesso hanno limitazioni in termini di memoria (Flash e RAM) e capacità di elaborazione. Queste limitazioni possono essere diverse tra differenti dispositivi, il che può rendere difficile la creazione di modelli di TinyML che funzionano indistiantamente per diverse piattaforme con risore limitate senza incorrere ad eccedere vincoli arhitetturali o senza compromettere l'accuratezza dei risultati.
Per evitare questi problemi, è opportuno per gli sviluppatori di TinyML implementare soluzioni che siano compatibili con il maggior numero possibile di dispositivi, il che può includere l'adozione di standard aperti e l'uso di architetture software flessibili e scalabili che consentano di adattarsi alle esigenze di un'ampia gamma di dispositivi.

Alcune tecniche che mirano alla creazione di modelli efficienti, pur mantenendo la flessibilità di adattamento a piattaforme con risorse computazionali stringenti, sono la compressione del modello tramite \textit{pruning} e \textit{quantizzazione}, lo \textit{scaling} (ad oggi lo stato dell'arte è il \textit{compound scaling}) e il \textit{neural architecture serach}. La prima pratica prevede di comprimere il modello già allenato in modo tale da ridurre le connessioni neurali eliminando (\textit{pruning}) quelle i cui pesi sono minori di una certa soglia e che quindi hanno un impatto irrilevante sul risultato finale di predizione; la \textit{quantizzazione} invece mira a ridurre lo spazio in memoria occupato dai pesi della rete riducendo la loro precisione in termini di bit (passando ad esempio da una codifica a 32bit ad una a 8bit). Lo \textit{scaling} si basa sul modificare alcune caratteristiche di una rete (numero di layer/risoluzione/canali) in modo da adattarla ai vincoli computazionali imposti cercando di preservare l'accuratezza di predizione; il \textit{neural architecture serach} invece è un nuovo ambito di ricerca molto complesso con l'intento di trovare la miglior architettura per una rete neurale per un compito specifico, automatizzando la ricerca e selezionando le caratteristiche della rete che sembrano portare a risultati migliori. 

%La tecnica del compound scaling è una strategia utilizzata nel TinyML per adattare la complessità dei modelli alle limitazioni di risorse dei dispositivi su cui verranno eseguiti. Tale tecninca prevede l'utilizzo di più iperparametri che regolano la complessità del modello, ovvero il numero di parametri e il numero di operazioni (FLOPs) richieste per l'esecuzione del modello. L'obiettivo è di trovare il giusto bilanciamento tra le risorse del dispositivo e la complessità del modello per ottenere le performance migliori in termini di accuratezza.

\newpage
\section{Scaling di Convolutional Neural Networks}

% %%%SPOSTATO SOPRA%%% Le Convolutional Neural Networks sono tipicamente composte da strati di convoluzione, pooling e strati completamente connessi e sono progettate appositamente per estrarre le caratteristiche salienti dalle immagini. Attraverso la convoluzione e l'applicazione di filtri, le CNN catturano dettagli significativi, riducendo la complessità dei dati, tramite gli strati di pooling, si riducono le dimensioni delle feature map per concentrarsi sulle informazioni essenziali, infine gli strati completamente connessi combinano le caratteristiche estratte per effettuare la classificazione. Grazie al processo di apprendimento e all'ottimizzazione dei pesi, le CNN sono in grado di affinare le loro prestazioni nel riconoscimento e nella classificazione di immagini.

Lo \textit{scaling} (ed il \textit{compound scaling}) è sicuramente la tecnica più utilizzata per implementare algoritmi efficienti di machine learning con poche esigenze di risorse. Le caratteristiche principali di una CNN che possono essere "scalate" sono la profondità, ovvero il numero di strati convolutivi presenti, la larghezza, ovvero il numero di canali delle rappresentazioni intermedie, e la risoluzione, ovvero le dimensioni in pixel delle immagini di input.

Per lo sviluppo di algoritmi di machine learning in grado di scalare è buona prassi implementre intrinsicamente nell'architettura della rete le tecniche di \textit{scaling}, singolarmente o combinate, in maniera personalizzabile, in modo tale da permettere la modifica delle caratteristiche della rete (tra cui memoria e potenza computazionale) tramite dei fattori di scala e generare quindi un modello ottimizzato per determinati dispositivi con risorse limitate. 

Abbiamo quindi appurato che tramite l'aggiustamento dei fattori di scala è possibile inizializzare una rete da allenare che soddisfi determinati vincoli di occupazione di memoria e di potenza computazionale; tuttavia ci sono diverse configurazioni dei fattori di scaling che ci portano ad avere un'architettura di rete differente, con le stesse esigenze di risorse ma con possibili risultati di accuratezza discrepanti, da qui nasce quindi il mio obiettivo di ricerca. \\

Dato un preciso budget di risorse computazionali, quale è la miglior configurazione dei fattori di scala che soddisfa i vincoli imposti e massimizza la performance di classificazione? \\

Per riuscire a rispondere a questa domanda bisogna innanzitutto selezionare un modello di rete di riferimento su cui condurre questa ricerca, ho scelto quindi la famiglia di reti delle PhiNet\cite{10.1145/3510832}, ovvero un'architettura scalabile ottimizzata per l'elaborazione delle immagini basata sul deep learning per piattaforme con risorse limitate.
Una volta individuata l'architettura bisogna essere in grado di comprendere quali sono le variabili che influenzano le performance finali (scelta del dataset, risoluzione delle immagini di input, utilizzo o meno della tecnica di data augmentation,  numero di epoche per l'addestramento, ottimizzatore utilizzato durante il training, valore del learning rate) e definire un sottodominio di ricerca con una precisa configurazione di tali varibili; questo restringimento del dominio è necessario per poter rendere l'analisi coerente e replicabile e non spostare il focus su fattori esterni all'ambito di ricerca. Bisogna inoltre comprendere come i fattori di scala influenzano la struttura della rete e riuscire ad analizzare la variazione delle performance al variare della scala applicata.

% TECNICHE UTILIZZATE

Le tecniche proposte per svolgere questo tipo di analisi variano da metodologie più "profonde", quali lo studio della separabilità lineare delle rappresentazioni intermedie per diverse tipologie di rete per comprendere l'impatto che gli strati convolutivi di mezzo hanno sulle performance finali (Linear Probing), fino a tecniche di analisi tramite regressione e segmentazione di funzioni per l'individuazione dei fattori di scala migliori per determinati budget di risorse. 
% RISUTATI E CONTRIBUTI
Attraverso questa dettagliata analisi, il presente studio offre un contributo personale nell'individuazione del numero di strati convoluzionali che ottimizza le performance di classificazione delle PhiNet per un certo budget computazionale e propone un framework di lavoro applicabile a diverse architetture per il TinyML, aprendo le strade a futuri studi.

%\newpage
-----------------------------

Sommario è un breve riassunto del lavoro svolto dove si descrive l'obiettivo, l'oggetto della tesi, le 
metodologie e le tecniche usate, i dati elaborati e la spiegazione delle conclusioni alle quali siete arrivati.  

Il sommario dell’elaborato consiste al massimo di 3 pagine e deve contenere le seguenti informazioni:
\begin{itemize}
  \item contesto e motivazioni 
  \item breve riassunto del problema affrontato
  \item tecniche utilizzate e/o sviluppate
  \item risultati raggiunti, sottolineando il contributo personale del laureando/a
\end{itemize}
